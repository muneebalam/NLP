{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "folder = '/Users/muneebalam/Desktop/Imperial/ML/project/scraped-emails/emails/'\n",
    "LIMIT = 30322\n",
    "req_lim = 100\n",
    "urlbase = 'https://www.wikileaks.org/clinton-emails/emailid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@asyncio.coroutine\n",
    "def get(*args, **kwargs):\n",
    "    response = yield from aiohttp.request('GET', *args, **kwargs)\n",
    "    return (yield from response.text())\n",
    "\n",
    "@asyncio.coroutine\n",
    "def extract_text(url, sem):\n",
    "    with (yield from sem):\n",
    "        page = yield from get(url)\n",
    "        w = open(get_save_file(url=url), 'w')\n",
    "        w.write(page)\n",
    "        w.close()\n",
    "        emailid = int(url[url.rfind('/')+1:])\n",
    "        if emailid % 1000 == 0:\n",
    "            print('Writing', emailid)\n",
    "\n",
    "def generate_links(start):\n",
    "    return [urlbase+str(x) for x in range(start, LIMIT+1) if not exists(get_save_file(x))]\n",
    "\n",
    "def get_save_file(emailid=None, url=None):\n",
    "    if emailid is None:\n",
    "        emailid = int(url[url.rfind('/')+1:])\n",
    "    return folder + str(emailid) + '.txt'\n",
    "\n",
    "@asyncio.coroutine\n",
    "def run(links):\n",
    "    sem = asyncio.Semaphore(req_lim)\n",
    "    fetchers = [extract_text(link, sem) for link in links]\n",
    "    return [(yield from f) for f in asyncio.as_completed(fetchers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "x = loop.run_until_complete(run(generate_links(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_file(emailid):\n",
    "    r = open(get_save_file(emailid))\n",
    "    data = r.read()\n",
    "    r.close()\n",
    "    \n",
    "    sent = 'n/a'\n",
    "    released = 'n/a'\n",
    "    sender = 'n/a'\n",
    "    recipient = 'n/a'\n",
    "    text = 'n/a'\n",
    "    subject = 'n/a'\n",
    "    \n",
    "    data = data[data.index('Share document'):]\n",
    "    \n",
    "    if 'Original Message' in data:\n",
    "        data = data[:data.index('Original Message')]\n",
    "    else:\n",
    "        data = data[:data.index('</div>')]\n",
    "    \n",
    "    i = data.index('From:')\n",
    "    data = data[i:]\n",
    "    sender = data[data.index('>')+1:data.index('</span>')].strip()\n",
    "    #print('From:\\t', sender)\n",
    "    \n",
    "    i = data.index('To:')\n",
    "    data = data[i:]\n",
    "    recipient = data[data.index('>')+1:data.index('</span>')].strip()\n",
    "    #print('To:\\t', recipient)\n",
    "    \n",
    "    i = data.index('Date:')\n",
    "    data = data[i:]\n",
    "    sent = data[5:data.index('Subject')].strip()\n",
    "    j = sent.index(' ')\n",
    "    sent, time = sent.split(' ')[:2]\n",
    "    hr, minute = [int(x) for x in time.split(':')]\n",
    "    y, m, d = [int(x.strip()) for x in sent.split('-')]\n",
    "    sent = datetime(y, m, d, hr, minute)\n",
    "    #print('Sent:\\t', sent)\n",
    "    \n",
    "    i = data.index('Subject:')\n",
    "    data = data[i:]\n",
    "    subject = data[8:data.index('</header>')].strip()\n",
    "    #print('Subject:\\t', subject)\n",
    "    \n",
    "    if emailid == 7632:\n",
    "        i = data.index('D ate:')\n",
    "    elif emailid == 9700:\n",
    "        i = data.index('D6a t e') + 2\n",
    "    elif emailid == 14704:\n",
    "        i = data.index('Date.')\n",
    "    elif emailid == 23805:\n",
    "        i = data.index('D a te :') + 3\n",
    "    else:\n",
    "        i = data.index('Date:')\n",
    "    data = data[i:]\n",
    "    released = data[5:data.index('</span>')].strip().replace(' ', '')\n",
    "    if released[-1] == '-':\n",
    "        released = released[:-1]\n",
    "    if released[0] == ':':\n",
    "        released = released[1:]\n",
    "    if len(released) > 10:\n",
    "        released = released[:10]\n",
    "    m, d, y = [int(x.strip()) for x in released.split('/')]\n",
    "    released = datetime(y, m, d)\n",
    "    #print('Released:\\t', released)\n",
    "    \n",
    "    data = data.split('</span>')[-1]\n",
    "    text = data.replace('<span class=\"inlinemeta\">', '').strip()\n",
    "    #print('Text:\\t', text)\n",
    "    \n",
    "    return sent, released, sender, recipient, subject, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting through 10000\n",
      "Done extracting through 20000\n",
      "Done extracting through 30000\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "skip = {15406}\n",
    "for i in range(LIMIT):\n",
    "    if (i+1) not in skip:\n",
    "        try:\n",
    "            data.append(parse_file(i+1))\n",
    "        except Exception as e:\n",
    "            print(i+1, e, e.args)\n",
    "            break\n",
    "    if (i+1) % 10000 == 0:\n",
    "        print('Done extracting through', i+1)\n",
    "df = pd.DataFrame.from_records(data)\n",
    "df.columns = ['Sent', 'Released', 'Sender', 'Recipient', 'Subject', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_nrc():\n",
    "    #returns {word: (positive/negative, {emotions})}\n",
    "    r = open('/Users/muneebalam/Desktop/Imperial/ML/project/NRC-Emotion-Lexicon/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt')\n",
    "    data = r.read().strip()\n",
    "    r.close()\n",
    "    \n",
    "    firstword = 'aback'\n",
    "    data = data[data.index(firstword):].split('\\n')\n",
    "    lex = {}\n",
    "    generalset = {'positive', 'negative'}\n",
    "    for line in data:\n",
    "        word, emot, posneg = line.split('\\t')\n",
    "        posneg = int(posneg)\n",
    "        if word not in lex:\n",
    "            lex[word] = [None, set()]\n",
    "        if posneg == 1:\n",
    "            if emot in generalset:\n",
    "                lex[word][0] = emot\n",
    "            else:\n",
    "                lex[word][1].add(emot)\n",
    "    return lex\n",
    "sent_dct = read_nrc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_word_lst = set(stopwords.words('english'))\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "opposites = {'anger': 'fear', 'anticipation': 'surprise', 'disgust': 'trust', 'fear': 'anger', \n",
    "             'joy': 'sadness', 'sadness': 'joy', 'surprise': 'anticipation', 'trust': 'disgust',\n",
    "            'positive': 'negative', 'negative': 'positive'}\n",
    "negation_words = {'not'}\n",
    "def split_by_words(text):\n",
    "    return tokens = word_tokenize(raw)\n",
    "def findemot(text):\n",
    "    emots = {'anger': 0, 'anticipation': 0, 'disgust': 0, 'fear': 0,\n",
    "        'joy': 0, 'sadness': 0, 'surprise': 0, 'trust': 0}\n",
    "    posnegneu = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "    wordcount = 0\n",
    "    #https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions#Plutchik.27s_wheel_of_emotions\n",
    "    #emotions have opposites\n",
    "    try:\n",
    "        sentences = split_by_sentences(text)\n",
    "        if len(sentences) == 1:\n",
    "            negation = False\n",
    "            words = split_by_words(text)\n",
    "            for word in words:\n",
    "                wordcount += 1\n",
    "                if word in negation_words:\n",
    "                    negation = not negation\n",
    "                if word in stop_word_lst:\n",
    "                    pass\n",
    "                else:\n",
    "                    if word in sent_dct:\n",
    "                        if sent_dct[word][0] is None:\n",
    "                            posnegneu['neutral'] += 1\n",
    "                        else:\n",
    "                            if negation:\n",
    "                                posnegneu[opposites[sent_dct[word][0]]] += 1\n",
    "                            else:\n",
    "                                posnegneu[sent_dct[word][0]] += 1\n",
    "                        for em in sent_dct[word][1]:\n",
    "                            if negation:\n",
    "                                emots[opposites[em]] += 1\n",
    "                            else:\n",
    "                                emots[em] += 1\n",
    "                    else:\n",
    "                        word2 = SnowballStemmer(\"english\").stem(word)\n",
    "                        if word2 in sent_dct:\n",
    "                            if sent_dct[word2][0] is None:\n",
    "                                posnegneu['neutral'] += 1\n",
    "                            else:\n",
    "                                if negation:\n",
    "                                    posnegneu[opposites[sent_dct[word2][0]]] += 1\n",
    "                                else:\n",
    "                                    posnegneu[sent_dct[word2][0]] += 1\n",
    "                            for em in sent_dct[word2][1]:\n",
    "                                if negation:\n",
    "                                    emots[opposites[em]] += 1\n",
    "                                else:\n",
    "                                    emots[em] += 1\n",
    "        else:\n",
    "            sents = [findemot(sen) for sen in sentences]\n",
    "            for sen_pnn, sen_emot, wc in sents:\n",
    "                wordcount += wc\n",
    "                for sen in sen_pnn:\n",
    "                    posnegneu[sen] += sen_pnn[sen]\n",
    "                for sen in sen_emot:\n",
    "                    emots[sen] += sen_emot[sen]\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return posnegneu, emots, wordcount\n",
    "def extract_anger(dcts):\n",
    "    return dcts[1]['anger']\n",
    "def extract_anticipation(dcts):\n",
    "    return dcts[1]['anticipation']\n",
    "def extract_disgust(dcts):\n",
    "    return dcts[1]['disgust']\n",
    "def extract_fear(dcts):\n",
    "    return dcts[1]['fear']\n",
    "def extract_joy(dcts):\n",
    "    return dcts[1]['joy']\n",
    "def extract_sadness(dcts):\n",
    "    return dcts[1]['sadness']\n",
    "def extract_surprise(dcts):\n",
    "    return dcts[1]['surprise']\n",
    "def extract_trust(dcts):\n",
    "    return dcts[1]['trust']\n",
    "def extract_positive(dcts):\n",
    "    return dcts[0]['positive']\n",
    "def extract_negative(dcts):\n",
    "    return dcts[0]['negative']\n",
    "def extract_neutral(dcts):\n",
    "    return dcts[0]['neutral']\n",
    "def extract_wordcount(dcts):\n",
    "    return dcts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emots = {'anger': extract_anger, 'anticipation': extract_anticipation, \n",
    "         'disgust': extract_disgust, 'fear': extract_fear,\n",
    "        'joy': extract_joy, 'sadness': extract_sadness, \n",
    "         'surprise': extract_surprise, 'trust': extract_trust}\n",
    "posnegneu = {'positive': extract_positive, 'negative': extract_negative, \n",
    "             'neutral': extract_neutral}\n",
    "df['sentinfo'] = df['Text'].apply(findemot) \n",
    "for e in emots:\n",
    "    df[e] = df['sentinfo'].apply(emots[e])\n",
    "for e in posnegneu:\n",
    "    df[e] = df['sentinfo'].apply(posnegneu[e])\n",
    "df['wordcount'] = df['sentinfo'].apply(extract_wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop('sentinfo', axis=1, inplace=True)\n",
    "df.to_csv(folder + 'wikiemails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resentiment():\n",
    "    df = pd.read_csv(folder + 'wikiemails.csv')\n",
    "    emots = {'anger': extract_anger, 'anticipation': extract_anticipation, \n",
    "         'disgust': extract_disgust, 'fear': extract_fear,\n",
    "        'joy': extract_joy, 'sadness': extract_sadness, \n",
    "         'surprise': extract_surprise, 'trust': extract_trust}\n",
    "    posnegneu = {'positive': extract_positive, 'negative': extract_negative, \n",
    "                 'neutral': extract_neutral}\n",
    "    df['sentinfo'] = df['Text'].apply(findemot) \n",
    "    for e in emots:\n",
    "        df[e] = df['sentinfo'].apply(emots[e])\n",
    "    for e in posnegneu:\n",
    "        df[e] = df['sentinfo'].apply(posnegneu[e])\n",
    "    df['wordcount'] = df['sentinfo'].apply(extract_wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:53\n",
      "Sent:\t 2010-06-30 06:53:00\n",
      "00:57\n",
      "Sent:\t 2010-06-17 00:57:00\n",
      "19:30\n",
      "Sent:\t 2010-06-15 19:30:00\n",
      "10:09\n",
      "Sent:\t 2010-06-27 10:09:00\n",
      "06:25\n",
      "Sent:\t 2010-06-30 06:25:00\n",
      "20:37\n",
      "Sent:\t 2010-06-15 20:37:00\n",
      "01:30\n",
      "Sent:\t 2010-06-24 01:30:00\n",
      "00:38\n",
      "Sent:\t 2010-07-01 00:38:00\n",
      "01:13\n",
      "Sent:\t 2010-06-19 01:13:00\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    parse_file(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
